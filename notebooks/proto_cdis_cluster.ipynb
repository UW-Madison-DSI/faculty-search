{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "Kyle: I had an idea that I wanted to run past you. As you know DSI will move to the new CDIS building in about a year. There is some discussion about arranging the faculty from computer science, statistics, and biomedical informatics into interdisciplinary “pods”. They are trying to come up with some themes that would achieve this goal. It’s basically a clustering problem. I was thinking that it might be cool to use the vector store to aid in this.\n",
    "Possible approaches:\n",
    "make a few TSNE / PCA plots restricted to papers by faculty in those departments [either on the web or exported as html with plotly]\n",
    "make a few TSNE / PCA plots by faculty instead of by paper (using the mean or a barrycenter of their papers) [either on the web or exported as html with plotly]\n",
    "try a few clustering approaches on either 1) papers or 2) faculty\n",
    "attempt to use GPT to suggest categories similar to the taxonomy project for news feed.\n",
    "\n",
    "Kyle: I will send an email about the clustering project sometime soon. I’m curious if you had any plans for next steps beyond what you sent above. I think we talked about using ChatGPT to name the clusters.\n",
    "If you could attach some simple stats to the clusters it might help. E.g. Number of unique faculty, number of departments represented. Maybe a small table with faculty assigned to each cluster.\n",
    "If we wanted to try to find clusters that had some nicer properties (like mix of departments), I’m not sure the best way to go about it. We could make more clusters first and then try to merge nearby ones to maximize some metric (like number of departments with a min/max number of people in the cluster).\n",
    "\n",
    "\n",
    "Departments:\n",
    "1. Statistics\n",
    "1. CS\n",
    "1. BMI\n",
    "1. Information school\n",
    "\n",
    "Steps:\n",
    "1. Get all departments' faculty\n",
    "1. Get all papers from faculty\n",
    "1. Get all papers' embeddings\n",
    "1. Cluster all papers\n",
    "1. Define clusters and generate descriptions with a list people involved\n",
    "1. Maybe grouping multiple cluster together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all departments' faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from embedding_search.academic_analytics import get_units, get_faculties\n",
    "from embedding_search.vector_store import get_author\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(messages: list[dict]) -> dict:\n",
    "    \"\"\"Ask gpt with a data package.\n",
    "\n",
    "    Example input: [{\"role\": \"user\", \"content\": \"Hello world example in python.\"}]\n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "df = pd.read_parquet(\"data/cdis_clustering.parquet\")\n",
    "with open(\"data/cdis_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "\n",
    "# units = get_units()\n",
    "# for unit in units:\n",
    "#     print(f\"{unit['unit']['id']}: {unit['unit']['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_units = {\n",
    "#     \"28603\": \"Department of Computer Sciences\",\n",
    "#     \"28673\": \"Department of Statistics\",\n",
    "#     \"28591\": \"Department of Biostatistics and Medical Informatics\",\n",
    "#     \"28634\": \"Information School\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated faculty from each selected unit\n",
    "\n",
    "# faculty_list = []\n",
    "# for unit_id, name in selected_units.items():\n",
    "#     print(f\"{unit_id}: {name}\")\n",
    "#     unit_faculties = get_faculties(unit_id)\n",
    "\n",
    "#     sel_faculties = [f for f in unit_faculties if f[\"isNonFaculty\"] == False]\n",
    "#     for f in sel_faculties:\n",
    "#         f[\"unit\"] = name\n",
    "\n",
    "#     faculty_list.extend(sel_faculties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_unit_from_faculty_profile(id: int, faculty_list: list[dict]) -> str:\n",
    "#     \"\"\"Lookup unit from faculty profile.\"\"\"\n",
    "#     for faculty in faculty_list:\n",
    "#         if faculty[\"id\"] == id:\n",
    "#             return faculty[\"unit\"]\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-download all authors in selected departments/units (expansive, run once)\n",
    "\n",
    "# from crawl import download_all_authors_in_unit\n",
    "# for unit_id, name in selected_units.items():\n",
    "#     print(f\"Downloading unit in {name}\")\n",
    "#     download_all_authors_in_unit(unit=unit_id, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load downloaded data to memory\n",
    "# authors = []\n",
    "# for x in faculty_list:\n",
    "#     try:\n",
    "#         authors.append(get_author(x[\"id\"]))\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Author {x['id']} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all papers embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units = []\n",
    "# names = []\n",
    "# embeddings = []\n",
    "# article_titles = []\n",
    "# article_doi = []\n",
    "\n",
    "# # Collect useful information\n",
    "# for a in authors:\n",
    "#     embeddings.extend(a.articles_embeddings)\n",
    "#     for article in a.articles:\n",
    "#         article_titles.append(article.title)\n",
    "#         article_doi.append(article.doi)\n",
    "#         names.append(a.first_name + \" \" + a.last_name)\n",
    "\n",
    "#         # Get department/unit name\n",
    "#         try:\n",
    "#             # Prioritizing primary affiliation\n",
    "#             unit = selected_units[str(a.unit_id)]\n",
    "#         except KeyError:\n",
    "#             # If faculty's primary affiliation is not in selected units\n",
    "#             # use the unit's faculty list to determine affiliation\n",
    "#             unit = get_unit_from_faculty_profile(a.id, faculty_list)\n",
    "#         units.append(unit)\n",
    "# embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"data/cdis_embeddings.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Number of units: {len(units)}\")\n",
    "# print(f\"Number of authors: {len(names)}\")\n",
    "# print(f\"Number of articles: {len(article_titles)}\")\n",
    "# print(f\"Number of dois: {len(article_doi)}\")\n",
    "# print(f\"Number of embeddings: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster all papers (on full vector) and make 2d projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some basic 2d projections\n",
    "# projection_pca = PCA(n_components=2).fit_transform(embeddings)\n",
    "# projection_tsne = TSNE(n_components=2, random_state=0).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pack into dataframe\n",
    "# df_tsne = pd.DataFrame(projection_tsne, columns=[\"x_tsne\", \"y_tsne\"])\n",
    "# df_pca = pd.DataFrame(projection_pca, columns=[\"x_pca\", \"y_pca\"])\n",
    "# df = pd.concat([df_tsne, df_pca], axis=1)\n",
    "# df[\"title\"] = article_titles\n",
    "# df[\"doi\"] = article_doi\n",
    "# df[\"name\"] = names\n",
    "# df[\"department\"] = units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save df for later use\n",
    "# df.to_parquet(\"data/cdis_clustering.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can use Elbow method to determine the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(embeddings)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.plot(range(2, 20), sse)\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Sum of Squared Distances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem to have an obvious inflection point..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a experiment class to help with experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterExplore:\n",
    "    \"\"\"Clustering experiment manager.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: np.array,\n",
    "        df: pd.DataFrame,\n",
    "        n_clusters: int,\n",
    "        label: bool = False,\n",
    "    ) -> None:\n",
    "        self.embeddings = embeddings\n",
    "        self.df = df.copy()\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        # Run clustering\n",
    "        if \"cluster\" not in self.df.columns:\n",
    "            self._cluster(n_clusters)\n",
    "\n",
    "        # Get cluster label from GPT\n",
    "        if label:\n",
    "            self.get_clusters_label()\n",
    "\n",
    "    def get_clusters_label(self) -> None:\n",
    "        \"\"\"Get label for each cluster.\"\"\"\n",
    "\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            self._label_cluster(cluster_id)\n",
    "\n",
    "    def plot(self) -> alt.Chart:\n",
    "        return (self._plot_clusters() & self._plot_faculty()).resolve_scale(\n",
    "            color=\"independent\"\n",
    "        )\n",
    "\n",
    "    # def cluster_statistics(self) -> pd.DataFrame:\n",
    "    #     count_unique_faculty = (\n",
    "    #         self.df.groupby([\"cluster\", \"department\"])\n",
    "    #         .agg(n_faculty=(\"name\", \"nunique\"))\n",
    "    #         .reset_index()\n",
    "    #     )\n",
    "    #     return count_unique_faculty.pivot_table(\n",
    "    #         index=\"cluster\", rows=\"label\", columns=\"department\", values=\"n_faculty\", fill_value=0\n",
    "    #     )\n",
    "\n",
    "    def cluster_faculty(self, cluster: int) -> pd.DataFrame:\n",
    "        return (\n",
    "            self.df.query(f\"cluster == {cluster}\")\n",
    "            .groupby([\"department\", \"name\"])\n",
    "            .agg(n_articles=(\"title\", \"count\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    # Private methods\n",
    "    def _cluster(self, n: int) -> None:\n",
    "        \"\"\"K-means clustering.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n, random_state=0).fit(self.embeddings)\n",
    "        self.df[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "    def _label_cluster(self, cluster_id: int) -> None:\n",
    "        \"\"\"Use GPT to label a cluster.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"Try to give a topic name to describe all the publication below: \\n\\n {self._get_cluster_pub_titles(cluster_id)}\"\n",
    "            gpt_label = ask_openai([{\"role\": \"user\", \"content\": prompt}])\n",
    "            self.df.loc[self.df.cluster == cluster_id, \"label\"] = gpt_label\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "    def _get_cluster_pub_titles(self, cluster_id: int) -> str:\n",
    "        \"\"\"Get a list of publications in a cluster.\"\"\"\n",
    "\n",
    "        titles = self.df.query(f\"cluster == {cluster_id}\").title.to_list()\n",
    "        return \"\\n\\n \".join([t for t in titles if t is not None])\n",
    "\n",
    "    def _plot_clusters(self) -> alt.Chart:\n",
    "        self.cluster_select = alt.selection_point(fields=[\"cluster\"])\n",
    "\n",
    "        tooltip = [\n",
    "            \"title\",\n",
    "            \"cluster\",\n",
    "            \"doi\",\n",
    "            \"name\",\n",
    "            \"department\",\n",
    "        ]\n",
    "\n",
    "        if \"label\" in self.df.columns:\n",
    "            tooltip.append(\"label\")\n",
    "\n",
    "        chart = (\n",
    "            alt.Chart(self.df)\n",
    "            .mark_circle()\n",
    "            .encode(\n",
    "                x=f\"x_tsne\",\n",
    "                y=f\"y_tsne\",\n",
    "                color=\"cluster:N\",\n",
    "                opacity=alt.condition(\n",
    "                    self.cluster_select, alt.value(1), alt.value(0.2)\n",
    "                ),\n",
    "                tooltip=tooltip,\n",
    "            )\n",
    "            .add_params(self.cluster_select)\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (chart)\n",
    "            .properties(\n",
    "                width=1000,\n",
    "                height=600,\n",
    "            )\n",
    "            .interactive()\n",
    "        )\n",
    "\n",
    "    def _plot_faculty(self) -> alt.Chart:\n",
    "        return (\n",
    "            alt.Chart(self.df)\n",
    "            .mark_bar()\n",
    "            .encode(\n",
    "                x=alt.X(\"name:N\", sort=\"-y\"),\n",
    "                y=\"count()\",\n",
    "                color=\"department:N\",\n",
    "                tooltip=[\"department\", \"name\", \"count()\"],\n",
    "            )\n",
    "            .transform_filter(self.cluster_select)\n",
    "            .properties(\n",
    "                width=1000,\n",
    "                height=300,\n",
    "                title=\"Number of publication in selected cluster by faculty\",\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ClusterExplore(embeddings, df, 18)\n",
    "experiment.get_clusters_label()\n",
    "experiment.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faculty distribution in each cluster (by department)\n",
    "\n",
    "count_unique_faculty = (\n",
    "    experiment.df.groupby([\"cluster\", \"label\", \"department\"])\n",
    "    .agg(n_faculty=(\"name\", \"nunique\"))\n",
    "    .reset_index()\n",
    ")\n",
    "count_unique_faculty.pivot_table(\n",
    "    index=[\"cluster\", \"label\"], columns=\"department\", values=\"n_faculty\", fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faculty name in each cluster\n",
    "for i in range(18):\n",
    "    print({i: experiment.cluster_faculty(i).name.tolist()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
